<p>This post is a short discussion on the 
        <span class="wikipedia_tooltip"><a href="https://en.wikipedia.org/wiki/Leslie_Lamport" target="_blank">Leslie Lamport</a>
            <span class="wikipedia_summary">
            <a href="https://en.wikipedia.org/wiki/Leslie_Lamport" target="_blank" class="wikipedia_wordmark">
              <img src="https://upload.wikimedia.org/wikipedia/commons/b/bb/Wikipedia_wordmark.svg">
              <span class="wikipedia_icon"></span>
            </a>
            Leslie Barry Lamport (born February 7, 1941) is an American computer scientist and mathematician. Lamport is best known for his seminal work in distributed systems, and as the initial developer of the document preparation system LaTeX and the author of its first manual.
            </span>
        </span>'s paper &quot;<a class="reference external" href="http://research.microsoft.com/en-us/um/people/lamport/pubs/teaching-concurrency.pdf">Teaching Concurrency</a>&quot;.</p>
<p>Lamport's basic premise is that understanding the system the most important part, and engineers often muddy their
understanding with implementation details as soon as they start talking about programming languages suitable for
concurrency.</p>
<p>He even challenges engineers to come up with the solution for concurrency problems without using 
        <span class="wikipedia_tooltip"><a href="https://en.wikipedia.org/wiki/Semaphore_(programming)" target="_blank">Semaphore (programming)</a>
            <span class="wikipedia_summary">
            <a href="https://en.wikipedia.org/wiki/Semaphore_(programming)" target="_blank" class="wikipedia_wordmark">
              <img src="https://upload.wikimedia.org/wikipedia/commons/b/bb/Wikipedia_wordmark.svg">
              <span class="wikipedia_icon"></span>
            </a>
            In computer science, a semaphore is a variable or abstract data type used to control access to a common resource by multiple threads and avoid critical section problems in a concurrent system such as a multitasking operating system. Semaphores are a type of synchronization primitive. A trivial semaphore is a plain variable that is changed (for example, incremented or decremented, or toggled) depending on programmer-defined conditions.
            </span>
        </span>, 
        <span class="wikipedia_tooltip"><a href="https://en.wikipedia.org/wiki/Monitor_(synchronization)" target="_blank">Monitor (synchronization)</a>
            <span class="wikipedia_summary">
            <a href="https://en.wikipedia.org/wiki/Monitor_(synchronization)" target="_blank" class="wikipedia_wordmark">
              <img src="https://upload.wikimedia.org/wikipedia/commons/b/bb/Wikipedia_wordmark.svg">
              <span class="wikipedia_icon"></span>
            </a>
            In concurrent programming, a monitor is a synchronization construct that prevents threads from concurrently accessing a shared object's state and allows them to wait for the state to change.  They provide a mechanism for threads to temporarily give up exclusive access in order to wait for some condition to be met, before regaining exclusive access and resuming their task.  A monitor consists of a mutex (lock) and at least one condition variable.  A condition variable is explicitly 'signalled' when the object's state is modified, temporarily passing the mutex to another thread 'waiting' on the condition variable.
            </span>
        </span>, or any other construct that were invented and provided as a tool. Using those, he says, is like using the
'sort' to routine the language to implement a sorting algorithm.</p>
<blockquote>
<p>The modern field of concurrency started with Dijkstra's 1965 paper on the mutual exclusion problem. For most of the
1970s, one &quot;solved&quot; the mutual exclusion problem by using semaphores or monitors or conditional critical regions or
some other language construct. This is like solving the sorting problem by using a programming language with a sort
command. Most of your colleagues can explain how to implement mutual exclusion using a semaphore. How many of them
can answer the following question: Can one implement mutual exclusion without using lower-level constructs that,
like semaphores, assume mutually exclusive access to a resource?</p>
</blockquote>
<p>Lamport's approach to problem solving suggests, we need to think of computing problem as series of states instead of
series of steps. I think, series of steps tend to give some linearity the approach, while series of states tend to
indicate that sub-parts of the system can have multiple states and the next state each sub-part can take only depends
upon the current state and action that leads the state transition to the next state.</p>
<blockquote>
<p>It is more useful to think about states than sequences of steps because what a computing device does next depends on
its current state, not on what steps it took in the past.</p>
<p>To describe a computation we need to describe a sequence of states. More often, we need to describe the set of
computations that can be produced by some particular computing device, such as an algorithm. There is one method that
works in practice: describing a set of computations by</p>
<ol class="arabic simple">
<li><p>the set of all initial initial states and</p></li>
<li><p>A next-state relation that describes, for every state, the possible next states that is, the set of states
reachable from that state by a single step.</p></li>
</ol>
</blockquote>
<p>On computational thinking.</p>
<blockquote>
<p>How should we describe computations?</p>
<p>Most computer scientists would probably interpret this question to mean, what language should we use? Imagine an
art historian answering &quot;how would you describe impressionist painting?&quot; by saying &quot;in French&quot;.</p>
<p>Programming and hardware-design languages don't help an engineer understand what problem a system should solve.
Thinking of computations as sequences of states, rather than as something described by a language, is the first
step towards such understanding.</p>
</blockquote>
<p>Lamport also describes in great details about importance of problem specification. Sometimes, when the problem is
specified clearly and understood problem, the solution becomes easy.  Most of our struggles seems to be with coming to
grasp the problem specification.</p>
<blockquote>
<p>The great contribution of Dijkstra's paper on mutual exclusion was not his solution; it was stating the problem.
(It is remarkable that, in this first paper on the subject, Dijkstra stated all the requirements that distinguish
mutual exclusion from fundamentally simpler and less interesting problems.)</p>
</blockquote>
<p><em>On concurrency, itself</em></p>
<p>He gives an example of concurrency problem  that according to him is &quot;trivial&quot;. It took me some reading to understand
the problem. <a class="reference external" href="http://stackoverflow.com/questions/24989756/what-is-the-inductive-invariant-of-the-simple-concurrent-program">StackOverflow.com</a> certainly helped.</p>
<blockquote>
<p>Once an engineer understands what a computation is and how it is described, she can understand the most important
concept in concurrency: invariance. A computing device does the correct thing only because it maintains a correct
state. Correctness of the state is expressed by an invariant—a predicate that is true in every state of every
computation.</p>
<p>Invariance is the key to understanding concurrent systems, but few engineers or computer scientists have learned to
think in terms of invariants. Here is a simple example.</p>
</blockquote>
<p><strong>Now, the problem</strong></p>
<p>Consider N processes numbered from 0 through N − 1 in which each process i executes</p>
<div class="math">
\begin{equation*}
x[i] :=1;
\end{equation*}
</div>
<div class="math">
\begin{equation*}

y[i] := x[(i-1)modN]
\end{equation*}
</div>
<p>and stops, where each <span class="math">\(x[i]\)</span> initially equals 0. (The reads and writes of each <span class="math">\(x[i]\)</span> are assumed to be
atomic.)</p>
<p>This algorithm satisfies the following property: after every process has stopped, <span class="math">\(y[i]\)</span> equals 1 for at least
one process <span class="math">\(i\)</span> .</p>
<p>It is easy to see that the algorithm satisfies this property; the last process <span class="math">\(i\)</span> to write <span class="math">\(y[i]\)</span> must set
it to 1. But that process doesnt set <span class="math">\(y[i]\)</span> to 1 because it was the last process to write y.</p>
<p>What a process does depends only on the current state, not on what processes wrote before it. The algorithm satisfies
this property because it maintains an inductive invariant.</p>
<p><strong>Explanation</strong></p>
<p>The explanation on how <span class="math">\(y[i]\)</span> equals for 1 at least one process <span class="math">\(i\)</span> goes like this.</p>
<ol class="arabic simple">
<li><p>The <span class="math">\(x_s\)</span> model the following behavior: <span class="math">\(x[i]\)</span> is 1 if and only if the process <span class="math">\(i\)</span> has already run.</p></li>
<li><p>After all processes have completed, all <span class="math">\(x_s\)</span> are thus set to 1.</p></li>
<li><p>The <span class="math">\(y_s\)</span> are a bit trickier: <span class="math">\(y[i]\)</span> is set if <span class="math">\(x[i-1]\)</span> was set, that is, <span class="math">\(y[i]\)</span> is 1 if and
only if the predecessor of <span class="math">\(i\)</span> had already run when <span class="math">\(i\)</span> was doing its write to <span class="math">\(y\)</span>.</p></li>
</ol>
<p>I essentially to had resort to <a class="reference external" href="http://stackoverflow.com/questions/24989756/what-is-the-inductive-invariant-of-the-simple-concurrent-program">StackOverflow.com</a> post author's explanation to completely understand this.</p>
<p>The program invariant is:</p>
<blockquote>
<p>If a process has set <span class="math">\(y[i]\)</span>, it must already have set <span class="math">\(x[i]\)</span> to 1.
This is true regardless whether <span class="math">\(y[i]\)</span> is set to 0 or 1.</p>
<p>Proving this invariant is quite easy: In the beginning, none of the <span class="math">\(y_s\)</span> is set, so it holds trivially.
During program execution, each write to <span class="math">\(y[i]\)</span> is 
        <span class="wikipedia_tooltip"><a href="https://en.wikipedia.org/wiki/Sequence_point" target="_blank">Sequence point</a>
            <span class="wikipedia_summary">
            <a href="https://en.wikipedia.org/wiki/Sequence_point" target="_blank" class="wikipedia_wordmark">
              <img src="https://upload.wikimedia.org/wikipedia/commons/b/bb/Wikipedia_wordmark.svg">
              <span class="wikipedia_icon"></span>
            </a>
            In C and C++, a sequence point defines any point in a computer program's execution at which it is guaranteed that all side effects of previous evaluations will have been performed, and no side effects from subsequent evaluations have yet been performed. They are a core concept for determining the validity of and, if valid, the possible results of expressions. Adding more sequence points is sometimes necessary to make an expression defined and to ensure a single valid order of evaluation.
            </span>
        </span> a write to <span class="math">\(x[i]\)</span>. Therefore
the invariant also holds for every step of the program afterwards.</p>
</blockquote>
<p>The further reasoning goes like this.</p>
<blockquote>
<p>The last process to finish sets <span class="math">\(y[i]\)</span> to 1 because, by definition of being the last process, its
predecessor must have already finished execution at that point (ie. its y value is already set).</p>
<p>Which means, because of the invariant, its <span class="math">\(x\)</span> value (which determines the last process' <span class="math">\(y\)</span> value)
has to be 1.</p>
</blockquote>
<p>The alternate way to look at this problem can give some intuitive understanding too.</p>
<blockquote>
<p>You cannot find an execution order in which all <span class="math">\(y_s\)</span> are set to 0. Doing so would require all processes to
execute before their predecessor. However, since our processes are arranged in a ring (that is, if I follow the
predecessor chain I will eventually end up at my starting point again), this leads to the contradiction that at
least one process must have finished executing before it wrote to <span class="math">\(y\)</span>.</p>
</blockquote>
<p>To understand this concurrency problem, it requires some notion of syntax, a prior understanding of proving hypothesis,
and possibly discussing the problem and solution.</p>
<p>Trying to understand itself, I guess, is a progress.</p>
